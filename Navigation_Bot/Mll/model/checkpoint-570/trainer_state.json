{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 10.0,
  "eval_steps": 500,
  "global_step": 570,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.17543859649122806,
      "grad_norm": 1829.3272705078125,
      "learning_rate": 4.921052631578947e-05,
      "loss": 45.8181,
      "step": 10
    },
    {
      "epoch": 0.3508771929824561,
      "grad_norm": 11438.5654296875,
      "learning_rate": 4.8333333333333334e-05,
      "loss": 45.0334,
      "step": 20
    },
    {
      "epoch": 0.5263157894736842,
      "grad_norm": 1831.923583984375,
      "learning_rate": 4.7456140350877195e-05,
      "loss": 46.2514,
      "step": 30
    },
    {
      "epoch": 0.7017543859649122,
      "grad_norm": 1782.7056884765625,
      "learning_rate": 4.657894736842106e-05,
      "loss": 45.0845,
      "step": 40
    },
    {
      "epoch": 0.8771929824561403,
      "grad_norm": 1373.04638671875,
      "learning_rate": 4.570175438596491e-05,
      "loss": 45.5785,
      "step": 50
    },
    {
      "epoch": 1.0526315789473684,
      "grad_norm": 15459.3662109375,
      "learning_rate": 4.4824561403508774e-05,
      "loss": 45.5527,
      "step": 60
    },
    {
      "epoch": 1.2280701754385965,
      "grad_norm": 2547.220458984375,
      "learning_rate": 4.394736842105263e-05,
      "loss": 44.9378,
      "step": 70
    },
    {
      "epoch": 1.4035087719298245,
      "grad_norm": 1098.547119140625,
      "learning_rate": 4.30701754385965e-05,
      "loss": 43.6881,
      "step": 80
    },
    {
      "epoch": 1.5789473684210527,
      "grad_norm": 1199.3065185546875,
      "learning_rate": 4.219298245614035e-05,
      "loss": 44.2196,
      "step": 90
    },
    {
      "epoch": 1.7543859649122808,
      "grad_norm": 8063.6044921875,
      "learning_rate": 4.1315789473684214e-05,
      "loss": 44.3645,
      "step": 100
    },
    {
      "epoch": 1.9298245614035088,
      "grad_norm": 12677.0419921875,
      "learning_rate": 4.043859649122807e-05,
      "loss": 44.5903,
      "step": 110
    },
    {
      "epoch": 2.1052631578947367,
      "grad_norm": 1597.5191650390625,
      "learning_rate": 3.956140350877193e-05,
      "loss": 43.5415,
      "step": 120
    },
    {
      "epoch": 2.280701754385965,
      "grad_norm": 1516.470703125,
      "learning_rate": 3.868421052631579e-05,
      "loss": 43.4,
      "step": 130
    },
    {
      "epoch": 2.456140350877193,
      "grad_norm": 7137.6298828125,
      "learning_rate": 3.7807017543859654e-05,
      "loss": 41.3019,
      "step": 140
    },
    {
      "epoch": 2.6315789473684212,
      "grad_norm": 1396.4183349609375,
      "learning_rate": 3.692982456140351e-05,
      "loss": 42.9169,
      "step": 150
    },
    {
      "epoch": 2.807017543859649,
      "grad_norm": 3852.700439453125,
      "learning_rate": 3.605263157894737e-05,
      "loss": 40.5303,
      "step": 160
    },
    {
      "epoch": 2.982456140350877,
      "grad_norm": 2788.025390625,
      "learning_rate": 3.5175438596491226e-05,
      "loss": 41.7404,
      "step": 170
    },
    {
      "epoch": 3.1578947368421053,
      "grad_norm": 10613.818359375,
      "learning_rate": 3.429824561403509e-05,
      "loss": 41.4415,
      "step": 180
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 2427.4853515625,
      "learning_rate": 3.342105263157895e-05,
      "loss": 40.3019,
      "step": 190
    },
    {
      "epoch": 3.5087719298245617,
      "grad_norm": 994.2363891601562,
      "learning_rate": 3.254385964912281e-05,
      "loss": 40.8895,
      "step": 200
    },
    {
      "epoch": 3.6842105263157894,
      "grad_norm": 2029.892822265625,
      "learning_rate": 3.1666666666666666e-05,
      "loss": 38.8036,
      "step": 210
    },
    {
      "epoch": 3.8596491228070176,
      "grad_norm": 5086.90869140625,
      "learning_rate": 3.078947368421053e-05,
      "loss": 38.6575,
      "step": 220
    },
    {
      "epoch": 4.035087719298246,
      "grad_norm": 2914.264892578125,
      "learning_rate": 2.9912280701754386e-05,
      "loss": 39.2472,
      "step": 230
    },
    {
      "epoch": 4.2105263157894735,
      "grad_norm": 9794.236328125,
      "learning_rate": 2.9035087719298248e-05,
      "loss": 38.2005,
      "step": 240
    },
    {
      "epoch": 4.385964912280702,
      "grad_norm": 5093.69580078125,
      "learning_rate": 2.8157894736842106e-05,
      "loss": 36.9511,
      "step": 250
    },
    {
      "epoch": 4.56140350877193,
      "grad_norm": 4352.31884765625,
      "learning_rate": 2.7280701754385968e-05,
      "loss": 37.8329,
      "step": 260
    },
    {
      "epoch": 4.7368421052631575,
      "grad_norm": 1760.420654296875,
      "learning_rate": 2.6403508771929823e-05,
      "loss": 36.3202,
      "step": 270
    },
    {
      "epoch": 4.912280701754386,
      "grad_norm": 79232.2421875,
      "learning_rate": 2.5526315789473688e-05,
      "loss": 37.6857,
      "step": 280
    },
    {
      "epoch": 5.087719298245614,
      "grad_norm": 25096.82421875,
      "learning_rate": 2.4649122807017543e-05,
      "loss": 35.2558,
      "step": 290
    },
    {
      "epoch": 5.2631578947368425,
      "grad_norm": 2016.286376953125,
      "learning_rate": 2.3771929824561405e-05,
      "loss": 35.1615,
      "step": 300
    },
    {
      "epoch": 5.43859649122807,
      "grad_norm": 5854.853515625,
      "learning_rate": 2.2894736842105263e-05,
      "loss": 35.3151,
      "step": 310
    },
    {
      "epoch": 5.614035087719298,
      "grad_norm": 7643.2236328125,
      "learning_rate": 2.201754385964912e-05,
      "loss": 34.6107,
      "step": 320
    },
    {
      "epoch": 5.7894736842105265,
      "grad_norm": 4285.5400390625,
      "learning_rate": 2.1140350877192983e-05,
      "loss": 34.7123,
      "step": 330
    },
    {
      "epoch": 5.964912280701754,
      "grad_norm": 22287.490234375,
      "learning_rate": 2.0263157894736842e-05,
      "loss": 34.7243,
      "step": 340
    },
    {
      "epoch": 6.140350877192983,
      "grad_norm": 4571.77197265625,
      "learning_rate": 1.9385964912280704e-05,
      "loss": 35.1814,
      "step": 350
    },
    {
      "epoch": 6.315789473684211,
      "grad_norm": 3600.549560546875,
      "learning_rate": 1.8508771929824562e-05,
      "loss": 33.9474,
      "step": 360
    },
    {
      "epoch": 6.491228070175438,
      "grad_norm": 5638.810546875,
      "learning_rate": 1.763157894736842e-05,
      "loss": 34.0402,
      "step": 370
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 3044.904052734375,
      "learning_rate": 1.6754385964912282e-05,
      "loss": 34.1001,
      "step": 380
    },
    {
      "epoch": 6.842105263157895,
      "grad_norm": 5016.515625,
      "learning_rate": 1.587719298245614e-05,
      "loss": 33.0989,
      "step": 390
    },
    {
      "epoch": 7.017543859649122,
      "grad_norm": 3389.2626953125,
      "learning_rate": 1.5e-05,
      "loss": 32.7307,
      "step": 400
    },
    {
      "epoch": 7.192982456140351,
      "grad_norm": 4265.77197265625,
      "learning_rate": 1.412280701754386e-05,
      "loss": 33.1756,
      "step": 410
    },
    {
      "epoch": 7.368421052631579,
      "grad_norm": 2309.532470703125,
      "learning_rate": 1.3245614035087719e-05,
      "loss": 32.7978,
      "step": 420
    },
    {
      "epoch": 7.543859649122807,
      "grad_norm": 23326.080078125,
      "learning_rate": 1.2368421052631579e-05,
      "loss": 32.2951,
      "step": 430
    },
    {
      "epoch": 7.719298245614035,
      "grad_norm": 12514.5263671875,
      "learning_rate": 1.1491228070175439e-05,
      "loss": 32.0012,
      "step": 440
    },
    {
      "epoch": 7.894736842105263,
      "grad_norm": 2248.02587890625,
      "learning_rate": 1.0614035087719299e-05,
      "loss": 31.2959,
      "step": 450
    },
    {
      "epoch": 8.070175438596491,
      "grad_norm": 15940.31640625,
      "learning_rate": 9.736842105263157e-06,
      "loss": 30.8813,
      "step": 460
    },
    {
      "epoch": 8.24561403508772,
      "grad_norm": 7500.6259765625,
      "learning_rate": 8.859649122807017e-06,
      "loss": 31.367,
      "step": 470
    },
    {
      "epoch": 8.421052631578947,
      "grad_norm": 8500.244140625,
      "learning_rate": 7.982456140350877e-06,
      "loss": 30.4509,
      "step": 480
    },
    {
      "epoch": 8.596491228070175,
      "grad_norm": 3914.329345703125,
      "learning_rate": 7.1052631578947375e-06,
      "loss": 31.5,
      "step": 490
    },
    {
      "epoch": 8.771929824561404,
      "grad_norm": 5190.81396484375,
      "learning_rate": 6.228070175438597e-06,
      "loss": 30.7307,
      "step": 500
    },
    {
      "epoch": 8.947368421052632,
      "grad_norm": 6849.51025390625,
      "learning_rate": 5.350877192982457e-06,
      "loss": 31.2322,
      "step": 510
    },
    {
      "epoch": 9.12280701754386,
      "grad_norm": 2076.426513671875,
      "learning_rate": 4.473684210526316e-06,
      "loss": 31.5939,
      "step": 520
    },
    {
      "epoch": 9.298245614035087,
      "grad_norm": 7118.94677734375,
      "learning_rate": 3.5964912280701756e-06,
      "loss": 30.1481,
      "step": 530
    },
    {
      "epoch": 9.473684210526315,
      "grad_norm": 6512.64599609375,
      "learning_rate": 2.719298245614035e-06,
      "loss": 31.5464,
      "step": 540
    },
    {
      "epoch": 9.649122807017545,
      "grad_norm": 2986.35888671875,
      "learning_rate": 1.8421052631578946e-06,
      "loss": 29.7256,
      "step": 550
    },
    {
      "epoch": 9.824561403508772,
      "grad_norm": 3152.2216796875,
      "learning_rate": 9.649122807017545e-07,
      "loss": 31.2143,
      "step": 560
    },
    {
      "epoch": 10.0,
      "grad_norm": 4484.97314453125,
      "learning_rate": 8.771929824561404e-08,
      "loss": 32.1913,
      "step": 570
    }
  ],
  "logging_steps": 10,
  "max_steps": 570,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1542347575787520.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
