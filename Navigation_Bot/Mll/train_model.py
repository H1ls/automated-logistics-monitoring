import os
import json
import torch
import evaluate
from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq,
    EarlyStoppingCallback
)
from peft import get_peft_model, LoraConfig, TaskType

# --- –ü—É—Ç–∏ ---
MODEL_NAME = "cointegrated/rut5-base"  # –∏–ª–∏ "google/flan-t5-base"
DATASET_PATH  = os.path.join(os.path.dirname(__file__), "dataset.jsonl")
FEEDBACK_PATH = os.path.join(os.path.dirname(__file__), "feedback.jsonl")
OUTPUT_DIR    = os.path.join(os.path.dirname(__file__), "model")
LOG_DIR       = os.path.join(os.path.dirname(__file__), "logs")

# --- –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ ---
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"üöÄ –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –±—É–¥–µ—Ç –∑–∞–ø—É—â–µ–Ω–∞ –Ω–∞: {device}")

# --- –£—Ç–∏–ª–∏—Ç—ã ---
def load_jsonl(path):
    with open(path, "r", encoding="utf-8") as f:
        return [json.loads(l) for l in f if l.strip()]

def is_valid_output(obj):
    return (
        isinstance(obj, list)
        and all(isinstance(x, dict) and {"date","time","address"} <= set(x) for x in obj)
    )

# --- –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã ---
data = load_jsonl(DATASET_PATH)
if os.path.exists(FEEDBACK_PATH):
    data += load_jsonl(FEEDBACK_PATH)

cleaned = []
prompt = (
    "–ò–∑–≤–ª–µ–∫–∏ –≤—Å–µ –∞–¥—Ä–µ—Å–Ω—ã–µ –±–ª–æ–∫–∏ —Å –¥–∞—Ç–æ–π –∏ –≤—Ä–µ–º–µ–Ω–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON: "
)
for i, row in enumerate(data):
    # –ü–æ–∫–∞–∂–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª –¥–ª—è –ø–µ—Ä–≤—ã—Ö 3
    if i < 3:
        print("ORIG INPUT:", row["input"])
        print("ORIG OUTPUT:", row.get("output", ""))
    row["input"] = prompt + row["input"].strip()
    out = row.get("output", [])
    if is_valid_output(out):
        row["output"] = json.dumps(out, ensure_ascii=False)
        cleaned.append(row)
    else:
        print("‚õî –ü—Ä–æ–ø—É—â–µ–Ω –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π output:", row.get("input"))

raw_ds = Dataset.from_list(cleaned)
split = raw_ds.train_test_split(test_size=0.1)

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

def preprocess(ex):
    inp = tokenizer(ex["input"], truncation=True, padding="max_length", max_length=512)
    tgt = tokenizer(ex["output"], truncation=True, padding="max_length", max_length=512)
    inp["labels"] = tgt["input_ids"]
    return inp

tokenized = split.map(preprocess, batched=False, remove_columns=raw_ds.column_names)

# –í—ã–≤–æ–¥–∏–º 3 –ø—Ä–∏–º–µ—Ä–∞ –ø–æ—Å–ª–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
for j in range(3):
    print(f"== –¢–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–∏–º–µ—Ä {j+1} ==")
    print("Decoded input :", tokenizer.decode(tokenized['train'][j]['input_ids'], skip_special_tokens=True))
    print("Decoded output:", tokenizer.decode(tokenized['train'][j]['labels'], skip_special_tokens=True))

# --- –ú–µ—Ç—Ä–∏–∫–∞ BLEU ---
bleu = evaluate.load("bleu")
def post(text): return text.strip()
def compute_metrics(p):
    preds, labels = p
    dp = tokenizer.batch_decode(preds, skip_special_tokens=True)
    dl = tokenizer.batch_decode(labels, skip_special_tokens=True)
    dp = [post(x) for x in dp]
    dl = [post(x) for x in dl]
    return {"bleu": bleu.compute(predictions=dp, references=[[l] for l in dl])["bleu"]}

# --- –ú–æ–¥–µ–ª—å –∏ LoRA ---
# base_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)
if os.path.exists(OUTPUT_DIR):
    base_model = AutoModelForSeq2SeqLM.from_pretrained(OUTPUT_DIR).to(device)
else:
    base_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)

lora_cfg   = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q","v"],
    lora_dropout=0.1,
    bias="none",
    task_type=TaskType.SEQ_2_SEQ_LM
)
model = get_peft_model(base_model, lora_cfg)

# --- –ê—Ä–≥—É–º–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –ø–æ —ç–ø–æ—Ö–µ ---
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=4,
    num_train_epochs=10,
    logging_dir=LOG_DIR,
    logging_steps=10,
    report_to="none",
    disable_tqdm=False
)

# --- Trainer —Å EarlyStopping –ø–æ BLEU ---
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized["train"],
    eval_dataset=tokenized["test"],
    tokenizer=tokenizer,
    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),
    compute_metrics=compute_metrics,
    # callbacks=[EarlyStoppingCallback]  # <- –∑–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å
)
# --- –ó–∞–ø—É—Å–∫ ---
model.print_trainable_parameters()
trainer.train()

# --- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ ---
model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)
